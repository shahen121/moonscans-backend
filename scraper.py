import time
import random
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
from models import MangaCard, MangaDetails, Chapter

class AzoraScraper:
    def __init__(self, headless=False):
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…ØªØµÙØ­ Ø§Ù„Ù…ØªØ®ÙÙŠ Ø¨Ø£Ø¹Ù„Ù‰ Ø¯Ù‚Ø© Ù…Ø­Ø§ÙƒØ§Ø©"""
        options = uc.ChromeOptions()
        # Ù…Ø­Ø§ÙƒØ§Ø© Ù…Ø³ØªØ®Ø¯Ù… Ø­Ù‚ÙŠÙ‚ÙŠ
        options.add_argument('--start-maximized')
        options.add_argument('--disable-notifications')
        
        # ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…ØªØµÙØ­ (ÙŠØ³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø­Ø±Ùƒ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ø¥ØµØ¯Ø§Ø± 3.14)
        self.driver = uc.Chrome(options=options, headless=headless)
        self.wait = WebDriverWait(self.driver, 20)
        print("âœ… ØªÙ… ØªØ´ØºÙŠÙ„ Ù…Ø­Ø±Ùƒ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ØªØ®ÙÙŠ.")

    def _smart_scroll(self):
        """ØªÙ…Ø±ÙŠØ± Ø°ÙƒÙŠ Ù„Ø¶Ù…Ø§Ù† ØªØ­Ù…ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØµÙˆØ± Ø¨Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© (Lazy Load)"""
        height = self.driver.execute_script("return document.body.scrollHeight")
        for i in range(0, height, 800):
            self.driver.execute_script(f"window.scrollTo(0, {i});")
            time.sleep(0.2)
        self.driver.execute_script("window.scrollTo(0, 0);")

    def get_manga_list(self, url="https://azoramoon.com/") -> List[MangaCard]:
        """Ø³Ø­Ø¨ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…Ø§Ù†Ø¬Ø§/Ø§Ù„Ù…Ø§Ù†Ù‡ÙˆØ§ Ù…Ù† Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
        print(f"ğŸ”„ Ø¬Ø§Ø±ÙŠ Ø³Ø­Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©...")
        self.driver.get(url)
        time.sleep(3)
        self._smart_scroll()
        
        soup = BeautifulSoup(self.driver.page_source, 'lxml')
        results = []
        
        # Ø§Ø³ØªÙ‡Ø¯Ø§Ù Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù‡ÙŠÙƒÙ„ÙŠØ© Ø§Ù„Ù…ÙˆÙ‚Ø¹
        items = soup.select('.bsx')
        for item in items:
            try:
                title_tag = item.select_one('a')
                title = title_tag.get('title')
                link = title_tag.get('href')
                
                # Ø¬Ù„Ø¨ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ© Ø¨Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©
                img_tag = item.select_one('img')
                img_url = img_tag.get('data-src') or img_tag.get('src')
                
                # Ø¢Ø®Ø± ÙØµÙ„
                latest_chap = item.select_one('.epxs').text.strip() if item.select_one('.epxs') else "N/A"
                
                results.append(MangaCard(
                    title=title,
                    url=link,
                    cover_image=img_url,
                    latest_chapter=latest_chap
                ))
            except Exception:
                continue
        return results

    def get_manga_details(self, url: str) -> MangaDetails:
        """Ø¬Ù„Ø¨ ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ù…Ø§Ù†Ø¬Ø§ØŒ Ø§Ù„ÙØµÙˆÙ„ØŒ ÙˆØ­Ø§Ù„Ø© Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©"""
        print(f"ğŸ” Ø¬Ø§Ø±ÙŠ ÙØ­Øµ Ø§Ù„ØªÙØ§ØµÙŠÙ„ ÙÙŠ: {url}")
        self.driver.get(url)
        time.sleep(2)
        
        # Ù…Ø­Ø§ÙˆÙ„Ø© ÙØªØ­ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ÙØµÙˆÙ„ ÙƒØ§Ù…Ù„Ø© Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…Ø®ÙÙŠØ©
        try:
            expand_btn = self.driver.find_element(By.CSS_SELECTOR, ".click-to-load")
            self.driver.execute_script("arguments[0].click();", expand_btn)
            time.sleep(1)
        except:
            pass

        soup = BeautifulSoup(self.driver.page_source, 'lxml')
        
        title = soup.select_one('.entry-title').text.strip()
        cover = soup.select_one('.thumb img').get('src')
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø­Ø§Ù„Ø©
        status = "ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ"
        info_elements = soup.select('.imptdt')
        for info in info_elements:
            if "Ø§Ù„Ø­Ø§Ù„Ø©" in info.text or "Status" in info.text:
                status = info.select_one('i').next_sibling.strip()

        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙØµÙˆÙ„
        chapters = []
        chapter_items = soup.select('#chapterlist li')
        for li in chapter_items:
            try:
                num = li.select_one('.chapternum').text.strip()
                date = li.select_one('.chapterdate').text.strip()
                link = li.select_one('a').get('href')
                
                chapters.append(Chapter(
                    number=num,
                    url=link,
                    date=date
                ))
            except:
                continue

        return MangaDetails(
            title=title,
            cover_image=cover,
            total_chapters=len(chapters),
            chapters=chapters,
            status=status
        )

    def close(self):
        self.driver.quit()
